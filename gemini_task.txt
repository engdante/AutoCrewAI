Анализирайки съдържанието на вашия скрипт run_crew.py и предходния лог файл run_debug.log, открих няколко технически и логически грешки, които трябва да коригирате за стабилна работа:

1. Критична грешка: Свързване с Ollama
В скрипта дефинирате OLLAMA_BASE_URL чрез променливи от средата. Логът обаче показва ConnectError: [WinError 10061], което означава, че скриптът не може да намери работещ сървър на localhost:11434.

Корекция: Уверете се, че приложението Ollama е стартирано преди скрипта. Ако Ollama работи на друг адрес, проверете вашия .env файл за правилните стойности на OLLAMA_SERVER и OLLAMA_PORT.

2. Грешка в пътя до лог файла при изключение
В блока except Exception as e имате следното съобщение: print(f"See {os.path.join(OUTPUT_DIR, 'debug.log')} for full technical details.").

Проблем: Във вашата функция setup_logging вие дефинирате името на файла като run_debug.log, а не debug.log.

Корекция: Променете текста в print на run_debug.log, за да не подвеждате потребителя къде да търси логовете.

3. Липсващ механизъм за рестарт на логването
Във функцията setup_logging премахвате съществуващите хендлъри, но ако run_crew се извика многократно в рамките на един и същ процес, logging.basicConfig няма да се преконфигурира повторно (тя работи само първия път).

Корекция: Вместо logging.basicConfig, използвайте по-стабилен метод за конфигуриране или добавете force=True параметър в basicConfig (наличен в Python 3.8+).

4. Потенциален проблем с "Manager Agent" при йерархична архитектура
В логиката за Process.hierarchical търсите мениджър по supervisor_agent_name в речника agents_dict.

Проблем: Вие пълните agents_dict използвайки agent.role като ключ. Ако в Crew.md под "Supervisor Agent" е записано името на агента (напр. "Legal Manager"), а неговата роля е описана по друг начин (напр. "Head of Legal Department"), скриптът няма да ги съвпадне и ще даде грешка.

Корекция: Уверете се, че стойността срещу - Supervisor Agent: в Crew.md съвпада точно с ролята (- **Role**:) на съответния агент.

5. Риск при "Injection 2" (Dynamic [[filename]])
Скриптът автоматично инжектира съдържанието на файлове, оградени в двойни скоби.

Проблем: Ако файлът е много голям (>50,000 символа), вие го съкращавате ("sampling"). Това е добре за контекста, но може да "счупи" правен или технически документ, като премахне важни клаузи в средата, без агентът да знае, че липсват ключови части.

Корекция: Помислете за добавяне на предупреждение в самия инжектиран текст, че документът е съкратен, за да може AI агентът да го вземе предвид при анализа.

6. Проблем с litellm и модела (от лога)
Логът показва, че litellm не познава ценовата структура за ollama/glm-4.7:fixed.

Корекция: Въпреки че това не спира скрипта, то пълни лога с предупреждения. Можете да добавите os.environ["LITELLM_LOGGING"] = "false", за да намалите излишния шум в run_debug.log.